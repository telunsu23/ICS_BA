# Dynamic Stealthy Backdoor Attack Against Anomaly Detectors in ICS

This repository contains the official code implementation for the paper **"Dynamic Stealthy Backdoor Attack Against Anomaly Detectors in Industrial Control Systems"**.

## ğŸ“ Introduction

Industrial Control Systems (ICS) are core components of modern critical infrastructure. While Deep Neural Network (DNN)-based anomaly detection methods have enhanced security, they also face threats from backdoor attacks. Existing backdoor methods typically use static triggers, which lack stealthiness in industrial scenarios and have limited attack effectiveness.

This project proposes a **dynamic stealthy backdoor attack** method tailored for ICS. This method achieves sample-adaptive trigger generation through an Encoder-Decoder architecture:

* **Encoder**: Encodes training set features into triggers and generates poisoned samples.

* **Decoder**: Reconstructs training set features from poisoned samples to ensure stealthiness.

* **Target**: Targets anomaly detection models.

## ğŸ“‚ File Structure

```
ICS_BA/
â”œâ”€â”€ ablation_study/       # Ablation study results (Lambda, Poison Ratio, Trigger Size)
â”œâ”€â”€ baseline/             # Baseline comparison algorithms (TSBA, BackTime ç­‰)
â”œâ”€â”€ dataset/              # Dataset directory (BATADAL, HAI, SWaT)
â”‚   â”œâ”€â”€ BATADAL/clean/    # Contains preprocessed scaler.pkl and hidden_info.json
â”‚   â””â”€â”€ ...
â”œâ”€â”€ model/                # Model definitions
â”‚   â”œâ”€â”€ decoder/          # Decoder network structure
â”‚   â”œâ”€â”€ detector/         # Anomaly detector
â”‚   â””â”€â”€ encoder/          # Encoder network structure (Trigger Generator)
â”œâ”€â”€ result/               # Experiment result save path
â”œâ”€â”€ utils/                # Utility functions (Data loading, config reading, plotting)
â”œâ”€â”€ config.yml            # Global configuration file
â”œâ”€â”€ trigger_generator_train.py  # [Step 1] Train trigger generator
â”œâ”€â”€ create_backdoored_dataset.py # [Step 2] Generate poisoned dataset
â”œâ”€â”€ attack.py             # [Step 3] Execute backdoor attack (Train victim model)
â”œâ”€â”€ attack_test.py        # [Step 4] Test attack effectiveness (ASR, BA)
â””â”€â”€ ...
```

## ğŸ› ï¸ Requirements

Please ensure the following dependencies are installed:

```
pip install -r requirements.txt
```

## ğŸš€ Quick Start

The attack process is mainly divided into three stages: training the trigger generator, generating poisoned data, and training/attacking the victim model.

### 1. Data Preparation

Please ensure the `dataset/` directory contains the cleaned data and standardization files (`scaler.pkl`) for the target datasets (BATADAL, SWaT, HAI).

### 2. Configuration Parameters

Modify the `config.yml` file to set experimental parameters:

* `dataset`: Select dataset ('BATADAL', 'SWaT', 'HAI')

* `poison_ratio`: Poisoning ratio (e.g., 0.1)

* `trigger_size`: Trigger size (alpha)

### 3. Run Attack Process

#### Step 1: Train Trigger Generator

Train the Encoder-Decoder network to learn how to generate stealthy and effective dynamic triggers.

```
python trigger_generator_train.py
```

* Model checkpoints will be saved in `result/<Dataset>/attack/trigger_generator.pth`ã€‚

#### Step 2: Create Backdoored Dataset

Use the trained generator to inject triggers into clean data.

```
python create_backdoored_dataset.py
```

#### Step 3: Launch Backdoor Attack Against Target Model

Train the anomaly detection model using the poisoned dataset.

```
python attack.py
```

#### Step 4: Evaluation

Evaluate Attack Success Rate (ASR) and Benign Accuracy (BA).

```
python attack_test.py
```


